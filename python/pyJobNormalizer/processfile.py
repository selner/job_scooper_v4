from nltk.corpus import stopwords
import unicodecsv
from nltk.stem.snowball import SnowballStemmer
snowstemmer = SnowballStemmer("english")
stopwrds = stopwords.words('english')



states = {
        'AK': 'Alaska',
        'AL': 'Alabama',
        'AR': 'Arkansas',
        'AS': 'American Samoa',
        'AZ': 'Arizona',
        'CA': 'California',
        'CO': 'Colorado',
        'CT': 'Connecticut',
        'DC': 'District of Columbia',
        'DE': 'Delaware',
        'FL': 'Florida',
        'GA': 'Georgia',
        'GU': 'Guam',
        'HI': 'Hawaii',
        'IA': 'Iowa',
        'ID': 'Idaho',
        'IL': 'Illinois',
        'IN': 'Indiana',
        'KS': 'Kansas',
        'KY': 'Kentucky',
        'LA': 'Louisiana',
        'MA': 'Massachusetts',
        'MD': 'Maryland',
        'ME': 'Maine',
        'MI': 'Michigan',
        'MN': 'Minnesota',
        'MO': 'Missouri',
        'MP': 'Northern Mariana Islands',
        'MS': 'Mississippi',
        'MT': 'Montana',
        'NA': 'National',
        'NC': 'North Carolina',
        'ND': 'North Dakota',
        'NE': 'Nebraska',
        'NH': 'New Hampshire',
        'NJ': 'New Jersey',
        'NM': 'New Mexico',
        'NV': 'Nevada',
        'NY': 'New York',
        'OH': 'Ohio',
        'OK': 'Oklahoma',
        'OR': 'Oregon',
        'PA': 'Pennsylvania',
        'PR': 'Puerto Rico',
        'RI': 'Rhode Island',
        'SC': 'South Carolina',
        'SD': 'South Dakota',
        'TN': 'Tennessee',
        'TX': 'Texas',
        'UT': 'Utah',
        'VA': 'Virginia',
        'VI': 'Virgin Islands',
        'VT': 'Vermont',
        'WA': 'Washington',
        'WI': 'Wisconsin',
        'WV': 'West Virginia',
        'WY': 'Wyoming'
}


def writedicttocsv(csvFileName, data, keys=None):
    """

    Args:
        csvFileName:
        data:
        keys:

    Returns:

    """
    print "Writing " + str(len(data)) + " rows to file " + csvFileName +"..."
    if keys is None:
        item = data.itervalues().next()
        keys = item.keys()

    csvfile = open(csvFileName, "wb")
    csv_writer = unicodecsv.DictWriter(csvfile, fieldnames=keys, dialect=unicodecsv.excel)
    csv_writer.writeheader()
    for row in data:
        for k in data[row].keys():
            if k not in keys:
                del data[row][k]
        try:
            csv_writer.writerow(data[row])
        except Exception:
            pass

    csvfile.close()
    return csvFileName

def loadCSV(csvFileName, rowKeyName = None):
    """

    Args:
        csvFileName:
        rowKeyName:

    Returns:

    """

    print "Loading " + csvFileName
    csv_fp = open(csvFileName, "rbU")
    dictRecords = {}
    fields = {}

    csv_reader = None
    try:
        with csv_fp:
            csv_reader = unicodecsv.DictReader(csv_fp, delimiter=",", quoting=unicodecsv.QUOTE_ALL, errors='strict')
            fields = csv_reader.fieldnames
            for row in csv_reader:
                if rowKeyName is None:
                    rowKeyName = fields[0]

                dictRecords[row[rowKeyName]] = row
    except Exception as err:
        print err
        pass

    print "Loaded " + str(len(dictRecords)) + " rows from " + csvFileName

    return { 'fieldnames' : fields, 'dict' : dictRecords }

import os
filepath = os.path.dirname(os.path.abspath(__file__)) # /a/b/c/d/e

abbrevfile = os.path.join(filepath, "static", "job-title-abbreviations.csv")
expandWords = loadCSV(abbrevfile, "abbreviation")['dict']

def tokenizeStrings(listData, field, fieldTokenized = "tokenized", retType="string"):
    """

    Args:
        listData:
        field:
        fieldTokenized:

    Returns:

    """

    for k in listData.keys():
        if len(k) == 0:
            print "String value for key was empty.  Skipping..."
            continue

        tokens = getScrubbedStringTokens(listData[k][field])

        if retType == "list":
            listData[k][fieldTokenized] = tokens

        elif retType == "set":
                listData[k][fieldTokenized] = set(tokens)
        else:
            # if retType == "string" or retType is None:
            listData[k][fieldTokenized] = "|{}|".format("|".join(tokens))

    return listData

import nltk
import string
# NOTE:  Need to run the download once per machine to get the dictionaries
# nltk.download()

def removeStopWords(listwords):
    """

    Args:
        listwords:

    Returns:

    """
    retwords = [i for i in listwords if i not in stopwrds]
    return retwords


def getStemmedWords(listwords):
    """

    Args:
        listwords:

    Returns:

    """
    retwords = [snowstemmer.stem(i) for i in listwords]
    return retwords

import codecs

exclude = set(codecs.encode(string.punctuation, "utf-8"))

import operator
def combine_dicts(a, b):
    """

    Args:
        a:
        b:

    Returns:

    """
    z = a.copy()
    for k in a.keys():
        for kb in b[k]:
            z[k][kb] = b[k][kb]
    return z

def getExpandedWords(strWords):
    """

    Args:
        strWords:

    Returns:

    """
    if not isinstance(strWords, basestring):
        strWords = str(strWords)
    assert(len(strWords) > 0)
    s = ''.join(ch for ch in strWords if ch not in exclude)

    retWords = []
    words = nltk.word_tokenize(s)
    for i in words:
        loweri = i.strip().lower()
        if loweri in expandWords:
            retWords.append(expandWords[loweri]['expansion'])
        else:
            retWords.append(loweri)

    retWords = nltk.word_tokenize(" ".join(retWords))
    return retWords

def getScrubbedStringTokens(inputstring):
    """

    Args:
        inputstring:

    Returns:

    """
    if not inputstring:
        return ""
    strNoAbbrev = getExpandedWords(inputstring)
    lTokensNoStop = removeStopWords(strNoAbbrev)
    lStemmedTokens = getStemmedWords(lTokensNoStop)

    return lStemmedTokens



def tokenizeJSONFile(inputFile, outputFile, dataKey=None, indexKey=None):
    """

    Args:
        inputFile:
        outputFile:
        dataKey:
        indexKey:

    Returns:

    """
    if indexKey is None:
        indexKey = 0
    if dataKey is None:
        dataKey = 0

    import json
    inf = open(inputFile, "r")
    inputData = json.load(inf)
    if inputData:
        print "Processing file " + inputFile
        if(isinstance(inputData, dict)):
            if('jobslist' in inputData and isinstance(inputData['jobslist'], dict) and len(inputData['jobslist']) > 0):
                outData = tokenizeStrings(inputData['jobslist'], dataKey, str(dataKey) + "Tokens")
                inputData[u'jobslist'] = outData
                outf = open(outputFile, "w")
                json.dump(inputData, outf, indent=4, encoding='utf-8')
                outf.close()
                print "Tokenized results written to " + outputFile
                return outputFile
    else:
            print "Error:  No job listings found in " + inputFile
    return None

def tokenizeFile(inputFile, outputFile, dataKey=None, indexKey=None):
    """

    Args:
        inputFile:
        outputFile:
        dataKey:
        indexKey:

    Returns:

    """
    if indexKey is None:
        indexKey = 0
    if dataKey is None:
        dataKey = 0

    data = loadCSV(inputFile, indexKey)
    fields = data['fieldnames']
    dictData = data['dict']
    dictStrings = {}
    # if (isinstance(dictData, dict) and len(dictData) > 0):
    #     for k, v in dictData.items():
    #         dictStrings[k] = v[dataKey]
        # print k, v, "\n"
        # print v[dataKey], "\n", "\n"
#    listStrings = [k, v[dataKey] for k, v in dictData.items()]
    tokenkey = str(dataKey) + "tokenized"
    outData = tokenizeStrings(dictData, dataKey, tokenkey)
    fields.append(tokenkey)
    writedicttocsv(outputFile, outData, fields)

    return outData

#
# def addMatchesToList(source, new_links, itemlist, out_folder, kind):
#     """
#
#     :rtype : object
#     """
#     if new_links is None:
#         new_links = []
#
#     for link in new_links:
#         item = dict(link.attrs.copy())
#         item['text'] = link.text.encode('ascii', 'ignore').lower()
#         item['words'] = removeStopWords(item['text'])
#         l = []
#         for w in item['words']:
#             l.append(w.encode('ascii', 'ignore'))
#         item['words'] = l
#         item['words_stemmed'] = getStemmedWords(item['words'])
#         item['source'] = source.lower()
#
#         itemlist.append(item)
#
#     writelisttocsv(os.path.join(out_folder, (source + "-" + kind +"titles.tsv")), itemlist)
#
#     countWords(itemlist, "words", out_folder, source, kind)
#     countWords(itemlist, "words_stemmed", out_folder, source, kind)
#
#     return itemlist
#
